{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous normalizing flows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "import jax\n",
    "\n",
    "from jax import config\n",
    "config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "import jax.numpy as np\n",
    "import flax.linen as nn\n",
    "import optax\n",
    "import diffrax as dfx\n",
    "import math\n",
    "from tensorflow_probability.substrates import jax as tfp\n",
    "\n",
    "from sklearn import datasets, preprocessing\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import trange"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 10_000\n",
    "\n",
    "x, _ = datasets.make_moons(n_samples=n_samples, noise=.06)\n",
    "\n",
    "scaler = preprocessing.StandardScaler()\n",
    "x = scaler.fit_transform(x)\n",
    "\n",
    "plt.hist2d(x[:, 0], x[:, 1], bins=100)\n",
    "plt.xlim(-2 ,2)\n",
    "plt.ylim(-2, 2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNFs\n",
    "\n",
    "The evolution of the log-density follows the instantaneous change-of-variables formula:\n",
    "$$\\frac{\\partial \\log p({z}(t))}{\\partial t}=-\\operatorname{Tr}\\left(\\frac{\\partial f}{\\partial {z}(t)}\\right)$$\n",
    "\n",
    "Get total change in log-density by integrating across time:\n",
    "$$\\log p_1\\left({z}\\left(t_1\\right)\\right)=\\log p_0\\left({z}\\left(t_0\\right)\\right)-\\int_{t_0}^{t_1} \\operatorname{Tr}\\left(\\frac{\\partial f}{\\partial {z}(t)}\\right) d t$$\n",
    "\n",
    "We can get an unbiased estimate of the trace of a matrix by taking a double product of that matrix with a noise vector.\n",
    "$$\\operatorname{Tr}(A)=E_{p({\\epsilon})}\\left[{\\epsilon}^T A {\\epsilon}\\right]$$\n",
    "\n",
    "Typically we'd also need to implement backprop-ing through an ODE with e.g. adjoints, but Diffrax will take care of this for us here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \"\"\" A simple MLP in Flax.\n",
    "    \"\"\"\n",
    "    hidden_dim: int = 32\n",
    "    out_dim: int = 2\n",
    "    n_layers: int = 3\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        for _ in range(self.n_layers):\n",
    "            x = nn.Dense(features=self.hidden_dim)(x)\n",
    "            x = nn.gelu(x)\n",
    "        x = nn.Dense(features=self.out_dim)(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "Adapted from [Diffrax](https://docs.kidger.site/diffrax/examples/continuous_normalising_flow/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logp_exact(t, y, args):\n",
    "    \"\"\" Compute trace directly.\n",
    "    \"\"\"\n",
    "    y, _ = y\n",
    "    _, func = args\n",
    "    t = np.atleast_1d(t)\n",
    "\n",
    "    fn = lambda y: func(np.concatenate([y, t]))  # Augmented function\n",
    "    f, f_vjp = jax.vjp(fn, y)  # VJPs can be computed at the ~same cost as computing f through reverse-mode AD\n",
    "\n",
    "    # Compute trace\n",
    "    (size,) = y.shape\n",
    "    (dfdy,) = jax.vmap(f_vjp)(np.eye(size))\n",
    "    logp = np.trace(dfdy)\n",
    "    return f, logp\n",
    "\n",
    "def logp_approx(t, y, args):\n",
    "    \"\"\" Approx. trace using Hutchinson's trace estimator.\n",
    "    \"\"\"\n",
    "    y, _ = y\n",
    "    z, func = args\n",
    "    t = np.atleast_1d(t)\n",
    "    \n",
    "    fn = lambda y: func(np.concatenate([y, t]))  # Augmented function\n",
    "    f, f_vjp = jax.vjp(fn, y) # VJPs can be computed at the ~same cost as computing f through reverse-mode AD\n",
    "    \n",
    "    # Trace estimator\n",
    "    (z_dfdy,) = f_vjp(z)\n",
    "    logp = np.sum(z_dfdy * z)\n",
    "    return f, logp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(0)\n",
    "t = np.ones((x.shape[0], 1))\n",
    "\n",
    "f = MLP(hidden_dim=64, out_dim=2, n_layers=3)\n",
    "params = f.init(key, np.concatenate([x, t], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = 0.0\n",
    "t1 = 1.0\n",
    "dt0 = 1e-2\n",
    "logp = 'exact'\n",
    "\n",
    "# Runs backward-in-time to train the CNF\n",
    "def loss_fn(params, y, f):\n",
    "    if logp == 'exact':\n",
    "        term = dfx.ODETerm(logp_exact)\n",
    "    elif logp == 'approx':\n",
    "        term = dfx.ODETerm(logp_approx)\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    solver = dfx.Heun()\n",
    "    eps = jax.random.normal(key, y.shape)\n",
    "    delta_log_likelihood = 0.0\n",
    "    y = (y, delta_log_likelihood)\n",
    "    func = lambda x: f.apply(params, x)\n",
    "    sol = dfx.diffeqsolve(term, solver, t1, t0, -dt0, y, (eps, func))\n",
    "    (z,), (delta_log_likelihood,) = sol.ys\n",
    "    log_prob = delta_log_likelihood + tfp.distributions.Normal(loc=0., scale=1.).log_prob(z).sum()\n",
    "    return - log_prob\n",
    "\n",
    "jax.vmap(loss_fn, in_axes=(None, 0, None))(params, x[:32], f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = optax.adamw(learning_rate=3e-4, weight_decay=1e-4)\n",
    "opt_state = opt.init(params)\n",
    "\n",
    "@partial(jax.jit, static_argnums=(2,))\n",
    "def loss_fn_vmapped(params, x_batch, f):\n",
    "    loss = jax.vmap(loss_fn, in_axes=(None, 0, None))(params, x_batch, f)\n",
    "    return loss.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 1000\n",
    "n_batch = 32\n",
    "\n",
    "with trange(n_steps) as steps:\n",
    "    for step in steps:\n",
    "\n",
    "        # Draw a random batches from x\n",
    "        key, _ = jax.random.split(key)\n",
    "        idx = jax.random.choice(key, x.shape[0], shape=(n_batch,))\n",
    "\n",
    "        x_batch = x[idx]\n",
    "        \n",
    "        loss, grads = jax.value_and_grad(loss_fn_vmapped)(params, x_batch, f)\n",
    "        updates, opt_state = opt.update(grads, opt_state, params)\n",
    "\n",
    "        params = optax.apply_updates(params, updates)\n",
    "\n",
    "        steps.set_postfix(val=loss)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_sample_fn(params, key, n_dim=2):\n",
    "    \"\"\" Produce single sample from the CNF by integrating forward.\n",
    "    \"\"\"\n",
    "    z = jax.random.normal(key, (n_dim,))\n",
    "    def func(t, x, args):\n",
    "        t = np.atleast_1d(t)\n",
    "        return f.apply(params, np.concatenate([x, t]))\n",
    "    term = dfx.ODETerm(func)\n",
    "    solver = dfx.Heun()\n",
    "    sol = dfx.diffeqsolve(term, solver, t0, t1, dt0, z)\n",
    "    (y,) = sol.ys\n",
    "    return y\n",
    "\n",
    "single_sample_fn(params, key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_fn = partial(single_sample_fn, params)\n",
    "\n",
    "n_samples = 100\n",
    "sample_key = jax.random.split(key, n_samples ** 2)\n",
    "x_sample = jax.vmap(sample_fn)(sample_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.hist2d(x_sample[:, 0], x_sample[:, 1], bins=100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-mps",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
